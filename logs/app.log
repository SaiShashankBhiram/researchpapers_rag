2025-05-03 00:30:58,750 - INFO - 🔌 Connecting to MongoDB...
2025-05-03 00:30:59,096 - INFO - 📥 Fetching documents from MongoDB...
2025-05-03 00:31:00,694 - INFO - ✅ Document #1 retrieved, length: 129809 chars
2025-05-03 00:31:00,694 - INFO - ✅ Document #2 retrieved, length: 5652 chars
2025-05-03 00:31:00,694 - INFO - 📄 Total valid documents retrieved: 2
2025-05-03 00:31:00,704 - INFO - 📦 Documents saved to 'data/documents.pkl'
2025-05-03 01:09:24,254 - INFO - 📂 Loaded 2 documents from pickle file.
2025-05-03 01:12:04,088 - INFO - 📂 Loaded 2 documents from pickle file.
2025-05-03 01:22:09,604 - INFO - 📂 Loaded 2 documents from pickle file.
2025-05-03 01:22:09,780 - WARNING - ⚠️ Skipping overly long chunk (129808 characters).
2025-05-03 01:22:11,447 - INFO - 🧩 Created 1 semantic chunks.
2025-05-03 01:22:12,874 - INFO - ✅ Vector embeddings saved to MongoDB.
2025-05-04 01:12:13,650 - INFO - 📂 Loaded 2 documents from pickle file.
2025-05-04 01:12:14,251 - WARNING - ⚠️ Skipping overly long chunk (129808 characters).
2025-05-04 01:12:16,186 - INFO - 🧩 Created 1 semantic chunks.
2025-05-04 01:12:17,692 - INFO - ✅ Vector embeddings saved to MongoDB.
2025-05-04 01:19:17,782 - INFO - 📂 Loaded 2 documents from pickle file.
2025-05-04 01:19:18,154 - WARNING - ⚠️ Skipping overly long chunk (129808 characters).
2025-05-04 01:19:19,764 - INFO - 🧩 Created 1 semantic chunks.
2025-05-04 01:19:21,073 - INFO - ✅ Vector embeddings saved to MongoDB.
2025-05-04 01:20:39,111 - INFO - 📂 Loaded 2 documents from pickle file.
2025-05-04 01:20:39,472 - WARNING - ⚠️ Skipping overly long chunk (129808 characters).
2025-05-04 01:20:41,137 - INFO - 🧩 Created 1 semantic chunks.
2025-05-04 01:20:42,339 - INFO - ✅ Vector embeddings saved to MongoDB.
2025-05-04 01:20:42,387 - INFO - ✅ Connected to MongoDB successfully.
2025-05-04 01:21:08,234 - INFO - 📂 Loaded 2 documents from pickle file.
2025-05-04 01:21:08,578 - WARNING - ⚠️ Skipping overly long chunk (129808 characters).
2025-05-04 01:21:10,414 - INFO - 🧩 Created 1 semantic chunks.
2025-05-04 01:21:11,944 - INFO - ✅ Vector embeddings saved to MongoDB.
2025-05-04 01:21:11,988 - INFO - ✅ Connected to MongoDB successfully.
2025-05-04 01:23:22,106 - INFO - 📂 Loaded 2 documents from pickle file.
2025-05-04 01:23:22,494 - WARNING - ⚠️ Skipping overly long chunk (129808 characters).
2025-05-04 01:23:24,050 - INFO - 🧩 Created 1 semantic chunks.
2025-05-04 01:23:25,060 - INFO - ✅ Vector embeddings saved to MongoDB.
2025-05-04 01:23:25,095 - INFO - ✅ Connected to MongoDB successfully.
2025-05-04 01:44:57,103 - INFO - 📂 Loaded 2 documents from pickle file.
2025-05-04 01:44:57,480 - WARNING - ⚠️ Skipping overly long chunk (129808 characters).
2025-05-04 01:44:59,236 - INFO - 🧩 Created 1 semantic chunks.
2025-05-04 01:45:00,358 - INFO - ✅ Vector embeddings saved to MongoDB.
2025-05-04 01:45:00,410 - INFO - ✅ Connected to MongoDB successfully.
2025-05-04 01:53:26,146 - INFO - 📂 Loaded 2 documents from pickle file.
2025-05-04 01:54:38,429 - INFO - 🧩 Created 152 text chunks using RecursiveCharacterTextSplitter.
2025-05-04 01:54:42,214 - INFO - ✅ Vector embeddings saved to MongoDB.
2025-05-04 01:56:54,227 - INFO - 📂 Loaded 2 documents from pickle file.
2025-05-04 01:58:07,812 - INFO - 🧩 Created 152 text chunks using RecursiveCharacterTextSplitter.
2025-05-04 01:58:12,654 - INFO - ✅ Vector embeddings saved to MongoDB.
2025-05-04 01:58:12,694 - INFO - ✅ Connected to MongoDB successfully.
2025-05-04 11:48:41,065 - INFO - 📂 Loaded 2 documents from pickle file.
2025-05-04 11:50:02,453 - INFO - 🧩 Created 152 text chunks using RecursiveCharacterTextSplitter.
2025-05-04 11:50:07,421 - INFO - ✅ Vector embeddings saved to MongoDB.
2025-05-04 11:50:07,470 - INFO - ✅ Connected to MongoDB successfully.
2025-05-04 12:13:28,840 - INFO - 📂 Loaded 2 documents from pickle file.
2025-05-04 12:14:42,081 - INFO - 🧩 Created 152 text chunks using RecursiveCharacterTextSplitter.
2025-05-04 12:14:47,652 - INFO - ✅ Vector embeddings saved to MongoDB.
2025-05-04 12:14:47,684 - INFO - ✅ Connected to MongoDB successfully.
2025-05-13 20:11:58,423 - INFO - 📂 Loaded 2 documents from pickle file.
2025-05-13 20:12:48,476 - INFO - 🧩 Created 152 text chunks using RecursiveCharacterTextSplitter.
2025-05-13 20:12:51,134 - INFO - ✅ Vector embeddings saved to MongoDB.
2025-05-13 20:12:51,179 - INFO - ✅ Connected to MongoDB successfully.
2025-05-15 13:38:03,559 - INFO - 📂 Loaded 2 documents from pickle file.
2025-05-15 13:39:03,931 - INFO - 📂 Loaded 2 documents from pickle file.
2025-05-15 13:39:59,597 - INFO - 🧩 Created 152 text chunks using RecursiveCharacterTextSplitter.
2025-05-15 13:40:02,343 - INFO - ✅ Vector embeddings saved to MongoDB.
2025-05-15 13:40:02,385 - INFO - ✅ Connected to MongoDB successfully.
2025-05-17 20:30:59,369 - INFO - 📂 Loaded 2 documents from pickle file.
2025-05-17 20:31:54,697 - INFO - 🧩 Created 152 text chunks using RecursiveCharacterTextSplitter.
2025-05-17 20:31:57,836 - INFO - ✅ Vector embeddings saved to MongoDB.
2025-05-17 20:31:57,878 - INFO - ✅ Connected to MongoDB successfully.
2025-05-17 20:32:47,672 - WARNING - ⚠️ Could not compute embedding similarity: could not convert string to float: np.str_("Based on the provided context, attention is a cognitive ability that enables concentration on some piece of incoming stimuli. It is something that we can summon up by will, but it's not the whole story. Even with clear intentions and goals, other aspects of the input can capture our attention and distract us if they are sufficiently salient. \n\nThe attention mechanism is not fully explained in the provided context, but it is mentioned that there are brain systems and mechanisms that underlie these attentional abilities. The context raises questions about how attention works, such as whether we actively inhibit and suppress distractions or simply ignore them, and what happens to the information we don't attend to. \n\nHowever, a clear definition or explanation of the attention mechanism is not provided in the given context. It seems that the chapter is setting the stage to explore attention as a cognitive ability and address four specific issues related to it, but the details of the attention mechanism itself are not elaborated upon.")
2025-05-17 20:32:47,672 - INFO - 🔍 Query: What is attention mechanism?
2025-05-17 20:32:47,672 - INFO - ✅ Expected: Attention mechanism helps models focus on important input parts.
2025-05-17 20:32:47,676 - INFO - 📝 Predicted: Based on the provided context, attention is a cognitive ability that enables concentration on some piece of incoming stimuli. It is something that we can summon up by will, but it's not the whole story. Even with clear intentions and goals, other aspects of the input can capture our attention and distract us if they are sufficiently salient.

In essence, attention is a mechanism that allows us to selectively focus on certain information while ignoring or suppressing other information that is not relevant to our current goals or tasks. This mechanism is not just a simple filtering process, but rather a complex cognitive ability that involves the interplay of multiple brain systems and mechanisms.

The context raises several questions about the attention mechanism, including whether we actively inhibit and suppress distractions or simply ignore them, what happens to the information we don't attend to, and what brain systems and mechanisms underlie attentional abilities. However, it does not provide a definitive answer to what the attention mechanism is, beyond describing it as a cognitive ability that enables concentration and selective focus.

Therefore, based on the provided context, attention mechanism can be understood as a complex cognitive process that enables us to selectively focus on certain information while ignoring or suppressing other information, but the exact underlying mechanisms and processes are not fully explained.
2025-05-17 20:32:47,676 - INFO - 📊 BLEU Score: 0.009029368377367225
2025-05-17 20:32:47,679 - INFO - 📊 ROUGE Scores: {'rouge1': 0.04291845493562232, 'rouge2': 0.017316017316017313, 'rougeL': 0.034334763948497854}
2025-05-17 20:32:47,679 - INFO - 📊 Cosine Similarity Score: None
2025-05-17 20:33:24,514 - WARNING - ⚠️ Could not compute embedding similarity: could not convert string to float: np.str_("I don't know. The provided context does not mention transformer architecture or provide any information related to it. The context appears to be discussing electrophysiology and human attention, specifically the development of technology in the 1960s to measure electrical activity in the brain and its relation to cognitive processes such as selective attention. There is no mention of transformer architecture or any concept related to artificial intelligence or machine learning.")
2025-05-17 20:33:24,514 - INFO - 🔍 Query: How does transformer architecture work?
2025-05-17 20:33:24,514 - INFO - ✅ Expected: Transformers use self-attention for efficient processing.
2025-05-17 20:33:24,514 - INFO - 📝 Predicted: I don't know. The provided context does not mention transformer architecture, and it appears to be discussing electrophysiology and human attention, specifically the measurement of electrical activity in the brain and its relation to cognitive processes such as selective attention. There is no mention of transformer architecture, which is a concept typically associated with artificial intelligence and natural language processing.
2025-05-17 20:33:24,518 - INFO - 📊 BLEU Score: 1.9257352961519595e-155
2025-05-17 20:33:24,518 - INFO - 📊 ROUGE Scores: {'rouge1': 0.08823529411764706, 'rouge2': 0.0, 'rougeL': 0.08823529411764706}
2025-05-17 20:33:24,518 - INFO - 📊 Cosine Similarity Score: None
2025-05-17 20:34:19,775 - WARNING - ⚠️ Could not compute embedding similarity: could not convert string to float: np.str_('In psychology, selective attention refers to the cognitive process by which the brain selectively focuses on certain stimuli, features, or tasks while ignoring others. This process is essential because there is too much information available at any given moment for our brains to process simultaneously. Selective attention allows us to prioritize and filter out irrelevant information, enabling us to concentrate on the most important or relevant stimuli.\n\nAccording to the provided context, selective attention can be driven by two types of factors: endogenous and exogenous. Endogenous factors are driven by our internal goals, such as searching for a particular friend or following an instruction. Exogenous factors, on the other hand, are driven by external stimuli that capture our attention, such as bright lights or loud noises.\n\nThe context also highlights that selective attention is not a random process. Rather, it is a mechanism that helps us to selectively process the most important information, and the information we miss is a direct result of this selection process. For instance, when we are searching for a friend at a party, we may not notice the posters on the wall because our attention is focused on finding our friend.\n\nIn summary, selective attention is a crucial cognitive process that enables us to focus on relevant information, filter out distractions, and make sense of our environment. It is a vital mechanism that helps us to navigate the world around us and achieve our goals.')
2025-05-17 20:34:19,777 - INFO - 🔍 Query: What is selective attention in psychology?
2025-05-17 20:34:19,777 - INFO - ✅ Expected: Selective attention refers to the brain's ability to focus on relevant information while filtering out distractions.
2025-05-17 20:34:19,777 - INFO - 📝 Predicted: Selective attention in psychology refers to the cognitive process by which an individual selectively focuses on certain aspects of their environment, stimuli, or information while ignoring or filtering out other aspects. This process is necessary because the human brain is capable of processing only a limited quantity of information in both space and time, and there is too much information available at any given moment for us to cope with.

According to the context, selective attention can be driven endogenously by our goals, such as searching for a particular friend or following an instruction, or exogenously by a salient or novel stimulus that captures attention away from the task at hand, such as a bright light or loud noise. This means that our attention can be directed voluntarily based on our internal goals and motivations or involuntarily by external stimuli that grab our attention.

The context also highlights that selective attention is not a random process, but rather a mechanism that selects the most important information for further processing. The type of information that we miss and the conditions under which we miss it are, therefore, the flip side of the cognitive processes involved in attentional selection. For example, being unaware of the posters on the wall at a party is a failure of selection that is a property of selectively searching for features of a friend.

Overall, selective attention is a crucial cognitive function that enables us to prioritize and process relevant information in our environment, while ignoring or filtering out irrelevant information.
2025-05-17 20:34:19,777 - INFO - 📊 BLEU Score: 0.03347257546141754
2025-05-17 20:34:19,777 - INFO - 📊 ROUGE Scores: {'rouge1': 0.0959409594095941, 'rouge2': 0.04460966542750929, 'rougeL': 0.0959409594095941}
2025-05-17 20:34:19,777 - INFO - 📊 Cosine Similarity Score: None
2025-05-17 20:34:19,777 - INFO - 📈 Completed batch evaluation!
2025-05-17 20:34:19,783 - INFO - 📊 Evaluation results saved successfully!
2025-05-17 20:35:37,213 - INFO - 📂 Loaded 2 documents from pickle file.
2025-05-17 20:36:43,995 - INFO - 🧩 Created 152 text chunks using RecursiveCharacterTextSplitter.
2025-05-17 20:36:47,118 - INFO - ✅ Vector embeddings saved to MongoDB.
2025-05-17 20:36:47,155 - INFO - ✅ Connected to MongoDB successfully.
